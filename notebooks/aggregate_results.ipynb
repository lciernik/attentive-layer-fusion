{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a15a1ad",
   "metadata": {},
   "source": [
    "## Notebook to aggregate single experiment results\n",
    "\n",
    "First part loads all results (in `results.json` files) from all experiments into a single dataframe.\n",
    "\n",
    "Second part filters the dataframe to keep only the best runs per experiment/dataset/model combination (based on validation balanced accuracy).\n",
    "\n",
    "Finally, different aggregated results dataframes are merged together, additional information columns are added, and absolute/relative performance columns are computed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aa48d8-0dd4-47d0-b680-97ac9b313f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from helper import add_additional_info, filter_df_for_best_runs, get_abs_rel_performance\n",
    "from constants import BASE_PATH_PROJECT, FOLDER_SUBSTRING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd47692-af6c-4026-9626-108276ea9b19",
   "metadata": {},
   "source": [
    "### Global variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e5e5fd-4d8e-4adc-837c-1d7fc0cd5aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_paths = [\n",
    "    # BASE_PATH_PROJECT / f\"results_{FOLDER_SUBSTRING}_exp\",\n",
    "    # BASE_PATH_PROJECT / f\"results_{FOLDER_SUBSTRING}_exp_wd0.1\",\n",
    "    # BASE_PATH_PROJECT / f\"results_{FOLDER_SUBSTRING}_exp_new_pcam\",\n",
    "    BASE_PATH_PROJECT / f\"results_{FOLDER_SUBSTRING}_rebuttal\",\n",
    "    BASE_PATH_PROJECT / f\"results_{FOLDER_SUBSTRING}_end2end_finetuning\",\n",
    "]\n",
    "\n",
    "FILTER_FOR_BEST_MODEL = True\n",
    "COMPUTE_RELATIVE_PERFORMANCES = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99af0df1-cb39-4748-8a6f-12a749cc2e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "storing_path = BASE_PATH_PROJECT / f\"results_{FOLDER_SUBSTRING}_exp/aggregated\"\n",
    "# storing_path = BASE_PATH_PROJECT / f\"results_{FOLDER_SUBSTRING}_rebuttal/aggregated\"\n",
    "# storing_path = BASE_PATH_PROJECT / f\"results_{FOLDER_SUBSTRING}_end2end_finetuning/aggregated\"\n",
    "storing_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OVERWRITE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae43f7c6-967a-452d-8fe5-ba0f00dc41bb",
   "metadata": {},
   "source": [
    "### Gather all results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8a71dc-67ae-4bb8-a6ad-7eec4220c5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_type_mapping = {\n",
    "    \"ep\": \"Efficient Probe\",\n",
    "    \"aim\": \"AIM\",\n",
    "    \"v-jepra\": \"V-JEPA\",\n",
    "    \"linear\": \"linear\",\n",
    "    \"cae\": \"cae\",\n",
    "    \"cae_nowq\": \"Ours-Wq\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_probe_type(res_path):\n",
    "    probe_type = res_path.absolute().as_posix().split(\"probe_type_\")[-1].split(\"/\")[0]\n",
    "    if not probe_type:\n",
    "        if \"linear_probe\" in res_path.absolute().as_posix():\n",
    "            probe_type = \"linear\"\n",
    "        else:\n",
    "            probe_type = \"cae\"\n",
    "    return probe_type_mapping[probe_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee971517-0a31-494f-a8a4-80a9e6764c06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for project_path in project_paths:\n",
    "    for res_path in project_path.rglob(\"seed_0/results.json\"):\n",
    "\n",
    "        df = pd.read_json(res_path)\n",
    "        if \"_unfrozen_premodel\" in str(res_path):\n",
    "            df[\"pipeline_type\"] = \"end2end_finetuning\"\n",
    "        elif \"_frozen_premodel\" in str(res_path):\n",
    "            df[\"pipeline_type\"] = \"end2end_probe\"\n",
    "        else:\n",
    "            df[\"pipeline_type\"] = \"feature_probe\"\n",
    "        df = add_additional_info(df)\n",
    "        model_id_n_hopt_slug = \"/\".join(res_path.parts[-11:-1])\n",
    "        df[\"model_id_n_hopt_slug\"] = model_id_n_hopt_slug\n",
    "        df[\"res_folder\"] = project_path.name\n",
    "        df[\"res_path\"] = res_path\n",
    "        df[\"probe_type\"] = get_probe_type(res_path)\n",
    "        res.append(df)\n",
    "all_results = pd.concat(res).reset_index(drop=True)\n",
    "all_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6866e9a4-8d85-48d5-bb2a-79d49c5f1963",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    all_results = all_results[\n",
    "        ~all_results[\"attention_dropout\"].isin([\"[0.1, 0.0]\", \"[0.3, 0.0]\"])\n",
    "    ].reset_index(drop=True)\n",
    "except KeyError:\n",
    "    print(\"No attentive probes included in all_results\")\n",
    "all_results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5354141d-00e0-4654-9772-1d15d801b5f6",
   "metadata": {},
   "source": [
    "### Post process the results\n",
    "\n",
    "- Select for each combination the best run based on the validation accuracy\n",
    "- Compute the relative performance gain compared to one layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130c0049-2978-481f-8195-9a5480edb56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bak = all_results.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ed1d7f-ef36-4b87-bb1a-134cdbd91d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = bak.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefcd0b8-ea54-4d80-8be5-261256333680",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FILTER_FOR_BEST_MODEL:\n",
    "    print(\"Filtering all runs ...\")\n",
    "    all_results = filter_df_for_best_runs(\n",
    "        df=all_results,\n",
    "        metric_col=\"best_val_bal_acc1\",\n",
    "        group_cols=[\"task\", \"probe_type\", \"experiment\", \"dataset\", \"model_ids\"],\n",
    "    )\n",
    "    all_results = all_results.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3788c3-5ec8-411d-8366-de2ee67419bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTE_RELATIVE_PERFORMANCES:\n",
    "    print(f\"{all_results.shape=} before computing relative performances\")\n",
    "    all_results = (\n",
    "        all_results.groupby([\"dataset\", \"base_model\"])\n",
    "        .apply(get_abs_rel_performance, include_groups=False)\n",
    "        .reset_index()\n",
    "    )\n",
    "    print(f\"{all_results.shape=} after computing relative performances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe0e1ef-a283-4037-a7e6-4bf333808eed",
   "metadata": {},
   "source": [
    "### Store the aggregated data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f9890b-3545-4a4d-a976-c7ef3ef708a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fn = storing_path / f\"all_runs_rebuttal.pkl\"\n",
    "if fn.exists() and not OVERWRITE:\n",
    "    raise FileExistsError(f\"File {fn} already exists. No overwriting!!\")\n",
    "else:\n",
    "    all_results.to_pickle(fn)\n",
    "    print(f\"Stored all aggregated results at {fn=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604826f1-a74a-4d54-8e00-4f45f77c0e61",
   "metadata": {},
   "source": [
    "#### Merge aggregated results\n",
    "\n",
    "Multiple aggregated results dataframes are merged together to form a single final dataframe of all experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c4418b-7f94-4eb7-9fcb-025acad64df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new runs\n",
    "all_runs_path = (\n",
    "    BASE_PATH_PROJECT / f\"results_{FOLDER_SUBSTRING}_rebuttal/aggregated/all_runs_rebuttal.pkl\"\n",
    ")\n",
    "print(all_runs_path)\n",
    "all_runs = pd.read_pickle(all_runs_path)\n",
    "print(all_runs.shape)\n",
    "\n",
    "# Load old runs\n",
    "prev_runs_path = BASE_PATH_PROJECT / f\"results_{FOLDER_SUBSTRING}_exp/aggregated/all_runs_v11.pkl\"\n",
    "print(prev_runs_path)\n",
    "prev_runs = pd.read_pickle(prev_runs_path)\n",
    "print(prev_runs.shape)\n",
    "\n",
    "# Load end2end runs\n",
    "runs_fine_tuning_path = (\n",
    "    BASE_PATH_PROJECT\n",
    "    / f\"results_{FOLDER_SUBSTRING}_end2end_finetuning/aggregated/all_runs_rebuttal.pkl\"\n",
    ")\n",
    "print(runs_fine_tuning_path)\n",
    "runs_fine_tuning = pd.read_pickle(runs_fine_tuning_path)\n",
    "print(runs_fine_tuning.shape)\n",
    "\n",
    "# Combine all runs\n",
    "all_runs = pd.concat([prev_runs, runs_fine_tuning, all_runs])\n",
    "all_runs = all_runs[~all_runs[\"dataset\"].isin([\"imagenet-subset-50k\"])].reset_index(\n",
    "    drop=True\n",
    ")\n",
    "\n",
    "print(\"Concatenated:\", all_runs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d30455-a336-4da7-9813-ad011222b3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_runs.loc[all_runs[\"dataset\"] == \"imagenet_torchvision\", \"dataset\"] = (\n",
    "    \"wds/imagenet1k\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50d4fa8-57cd-4106-9525-069c1c0d1eb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get relative performance\n",
    "result = all_runs.groupby([\"dataset\", \"base_model\"]).apply(\n",
    "    get_abs_rel_performance, include_groups=False\n",
    ")\n",
    "all_runs = result.reset_index(level=[0, 1])\n",
    "\n",
    "# Remove not assessed models\n",
    "all_runs = all_runs[~all_runs[\"dataset\"].isin([\"imagenet-subset-50k\"])].reset_index(\n",
    "    drop=True\n",
    ")\n",
    "\n",
    "# Add additional columns describing if it contains intermediate layers\n",
    "all_runs[\"contains_intermediate\"] = all_runs[\"model_ids\"].apply(\n",
    "    lambda x: len(\n",
    "        [elem for elem in eval(x) if elem.split(\"@\")[-1] not in [\"norm\", \"visual\"]]\n",
    "    )\n",
    "    > 0\n",
    ")\n",
    "all_runs.loc[all_runs[\"probe_type\"].isna(), \"probe_type\"] = np.where(\n",
    "    all_runs.loc[all_runs[\"probe_type\"].isna(), \"task\"].str.contains(\"linear\"),\n",
    "    \"linear\",\n",
    "    \"cae\",\n",
    ")\n",
    "all_runs.loc[all_runs[\"pipeline_type\"].isna(), \"pipeline_type\"] = \"feature_probe\"\n",
    "\n",
    "cols_distinct = list(\n",
    "    set(all_runs.columns)\n",
    "    - set(\n",
    "        [\n",
    "            \"hopt_time_hr\",\n",
    "            \"hopt_time_s\",\n",
    "            \"level_2\",\n",
    "            \"res_folder\",\n",
    "            \"res_path\",\n",
    "            \"test_data_inference__time_hr\",\n",
    "            \"test_data_inference_time\",\n",
    "            \"train_data_inference_time\",\n",
    "            \"train_data_inference_time_hr\",\n",
    "            \"training_time\",\n",
    "            \"training_time_hr\",\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "print(all_runs.shape)\n",
    "duplicated_tuns = all_runs[all_runs[cols_distinct].duplicated(keep=\"first\")].copy()\n",
    "all_runs = all_runs[~all_runs[cols_distinct].duplicated(keep=\"first\")]\n",
    "print(all_runs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53352d5f-ef95-4e68-b3da-a370f1ea466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = storing_path / f\"complete_set_of_run.pkl\"\n",
    "if fn.exists() and not OVERWRITE:\n",
    "    raise FileExistsError(f\"File {fn} already exists. No overwriting!!\")\n",
    "else:\n",
    "    all_runs.to_pickle(fn)\n",
    "    print(f\"Stored all aggregated results at {fn=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
