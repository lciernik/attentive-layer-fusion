{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9e1dd07",
   "metadata": {},
   "source": [
    "## Performance tables for each model on all task and probe type combinations\n",
    "\n",
    "The following notebook generates the performance tables for each model on all task and probe type combinations. It loads the aggregated results (`complete_set_of_run.pkl`) from the experiments and formats them into tables for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45e6113-57ca-416a-9644-b152011f534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('..')\n",
    "sys.path.append('../..')\n",
    "\n",
    "from constants import BASE_PATH_PROJECT, FOLDER_SUBSTRING, experiment_with_probe_type_order_list\n",
    "from helper import style_multimodel_heatmap, init_plotting_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca097eb-1d2d-475e-bb5c-220c16915e91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "init_plotting_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f93492-a1b6-4b56-8e0f-2c4af922cca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = 'both'\n",
    "\n",
    "base_storing_path = BASE_PATH_PROJECT / f\"results_{FOLDER_SUBSTRING}_rebuttal/plots\"\n",
    "if SAVE:\n",
    "    base_storing_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f314b30a-51d5-41c8-9048-13ea03bab450",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_runs= pd.read_pickle(BASE_PATH_PROJECT / f'results_{FOLDER_SUBSTRING}_rebuttal/aggregated/complete_set_of_run.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203d599e-bd7c-4dae-b272-e25c9f97772e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_runs = all_runs.drop(index=all_runs[(all_runs['nr_layers'] == 1) & all_runs['contains_intermediate']].index).copy().reset_index(drop=True)\n",
    "all_runs = all_runs[all_runs['probe_type'].isin(['cae', 'linear'])].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73d7ce6",
   "metadata": {},
   "source": [
    "### Some santity checks on number of runs per model/task/probe type combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7943c2d0-3055-461c-a27b-a098ff6517d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_runs = all_runs[['dataset', 'Experiment']].value_counts().sort_index().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6c8879-75dc-4e0a-98d8-9589f2bcd20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_runs_pivot = pd.pivot(\n",
    "    check_runs,\n",
    "    index = 'dataset',\n",
    "    columns = 'Experiment',\n",
    "    values= 'count'\n",
    ").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caac2b7-5450-45cd-ac1c-dd7bc099c296",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_runs_pivot = check_runs_pivot[experiment_with_probe_type_order_list].sort_index()\n",
    "if SAVE:\n",
    "    fn = base_storing_path / \"per_experiment_eval_count\" / 'check_runs_pivot.csv'\n",
    "    fn.parent.mkdir(parents=True, exist_ok=True)\n",
    "    check_runs_pivot.to_csv(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330900ae-844b-4ead-93da-f7cb542735da",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_runs_pivot_styled = check_runs_pivot.style.background_gradient(cmap='Reds_r', axis=None)\n",
    "check_runs_pivot_styled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66178303-4b7d-43de-9e56-9d82f0d0bef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_runs_pivot.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e34011-2df2-42d6-b57e-280a5a63d256",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_models = sorted(all_runs['base_model'].unique())\n",
    "selected_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a369a3d4-228a-4c5a-bee1-14e7821f661a",
   "metadata": {},
   "source": [
    "## Per model performance tables for all task and probe type combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a431899e-51c7-4d9c-ad7e-1625fccedda9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_runs_pivot = pd.pivot(\n",
    "    all_runs,\n",
    "    index = 'dataset_fmt',\n",
    "    columns = ['base_model','Experiment'],\n",
    "    values= 'test_lp_bal_acc1'\n",
    ")\n",
    "curr_col_order = list(product(selected_models, experiment_with_probe_type_order_list))\n",
    "diff = set(curr_col_order) - set(check_runs_pivot.columns.tolist())\n",
    "for col in diff:\n",
    "    check_runs_pivot.loc[:, col] = np.nan\n",
    "\n",
    "check_runs_pivot = check_runs_pivot.loc[:, curr_col_order].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e1029d-4d57-4a9c-b198-db43b7c41607",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_maps = [\n",
    "    'Greys', 'Purples', 'Blues', 'Greens', 'Oranges', 'Reds',\n",
    "    'YlOrBr', 'YlOrRd', 'OrRd', 'PuRd', 'RdPu', 'BuPu',\n",
    "    'GnBu', 'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn'\n",
    "    ]\n",
    "for i, model in enumerate(check_runs_pivot.columns.get_level_values(0).unique()):\n",
    "    tmp = check_runs_pivot.loc[:, [(model, col) for col in experiment_with_probe_type_order_list]].copy()\n",
    "    tmp *= 100\n",
    "    tmp2 = tmp.copy()\n",
    "    for row_idx, row_data in tmp2.iterrows():\n",
    "        if 'mae' in model:\n",
    "            tmp2.loc[row_idx, row_data.index] = row_data - row_data.loc[(model, 'AP last layer')]\n",
    "        else: \n",
    "            tmp2.loc[row_idx, row_data.index] = row_data - row_data.loc[(model, 'CLS last layer')]\n",
    "    \n",
    "    tmp.loc[\"min perf. gain\", :] = tmp2.min(skipna=True, axis=0)\n",
    "    tmp.loc[\"median perf. gain\", :] =  tmp2.median(skipna=True, axis=0)\n",
    "    tmp.loc[\"max perf. gain\", :] =  tmp2.max(skipna=True, axis=0)\n",
    "    tmp.loc[\"mean perf. gain\", :] =  tmp2.mean(skipna=True, axis=0)\n",
    "    tmp.loc[\"std perf. gain\", :] =  tmp2.std(skipna=True, axis=0)\n",
    "    \n",
    "    styled_df = style_multimodel_heatmap(tmp, color_maps=color_maps[i:(i+1)])\n",
    "    \n",
    "    if SAVE:\n",
    "        fn = base_storing_path / \"per_model_all_performances\" / f'perf_table_{model}.csv'\n",
    "        fn.parent.mkdir(parents=True, exist_ok=True)\n",
    "        tmp.to_csv(fn)\n",
    "        print(f\"Stored {model=} performance table at {fn=}.\")\n",
    "        print()\n",
    "    else:\n",
    "        display(styled_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
