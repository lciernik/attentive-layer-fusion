{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eba807c",
   "metadata": {},
   "source": [
    "## Attention heatmaps for trained attentive probes across layers for Masked Autoencoders\n",
    "This notebook visualizes the attention weights of learned attentive probes across different layers of Masked Autoencoders (MAE) models. It generates heatmaps to illustrate how attention is distributed across layers for various datasets, providing insights into the probe's focus during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c923ba9-ce9e-41d8-b5fb-9402f5444085",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "sys.path.append('..')\n",
    "sys.path.append('../..')\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib as mpl\n",
    "import pickle\n",
    "from functools import partial\n",
    "from src.data.feature_combiner import StackedZeroPadFeatureCombiner\n",
    "from src.data.data_loader import get_combined_feature_dl\n",
    "from src.utils.utils import load_model_from_file\n",
    "from src.utils.attention_utils import get_attention_weights\n",
    "from constants import base_model_name_mapping\n",
    "\n",
    "from constants import BASE_PATH_PROJECT, FOLDER_SUBSTRING, ds_info_file\n",
    "from helper import load_ds_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b318e9ce-4e7e-4e01-b0bf-c5d78962faed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsizes = { 'title': 14, 'legend': 13, 'label': 13, 'ticks': 12, }\n",
    "fontsizes_cols = { 'title': 18, 'legend': 17, 'label': 17, 'ticks': 16, }\n",
    "fontsizes_cols = { 'title': 24, 'legend': 23, 'label': 23, 'ticks': 23, }\n",
    "\n",
    "# Choose fontsizes or fontsizes_cols\n",
    "FS = fontsizes_cols  # or fontsizes_cols\n",
    "\n",
    "mpl.rcParams.update({\n",
    "    \"axes.titlesize\": FS[\"title\"],\n",
    "    \"axes.labelsize\": FS[\"label\"],\n",
    "    \"xtick.labelsize\": FS[\"ticks\"],\n",
    "    \"ytick.labelsize\": FS[\"ticks\"],\n",
    "    \"legend.fontsize\": FS[\"legend\"],\n",
    "    \"figure.titlesize\": FS[\"title\"],   # for plt.suptitle\n",
    "})\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n",
    "base_feature_dir = BASE_PATH_PROJECT / \"features\"\n",
    "#base_model_dir = BASE_PATH_PROJECT / f\"models_{FOLDER_SUBSTRING}\" \n",
    "base_model_dir = BASE_PATH_PROJECT / f\"models_{FOLDER_SUBSTRING}_rebuttal\" \n",
    "# helper: extract block number if present, otherwise put at the end\n",
    "def extract_block_key(s):\n",
    "    m = re.search(r'blocks\\.(\\d+)', s)\n",
    "    return int(m.group(1)) if m else 999  # \"norm\" goes last\n",
    "\n",
    "# --- Sort columns: CLS first, then AP, numeric order, 'last' at end ---\n",
    "def layer_sort_key(x):\n",
    "    x = str(x)\n",
    "    is_cls = \"cls\" in x\n",
    "    is_last = \"last\" in x\n",
    "    try:\n",
    "        num = int(x.split(\"@\")[-1]) if not is_last else 999\n",
    "    except ValueError:\n",
    "        num = 999\n",
    "    return (0 if is_cls else 1, num)\n",
    "\n",
    "def format_layer_label(label: str) -> str:\n",
    "    label = str(label)\n",
    "    if label.startswith(\"cls@\"):\n",
    "        return \"CLS \" + (\"Last\" if \"last\" in label else label.split(\"@\")[-1])\n",
    "    elif label.startswith(\"ap@\"):\n",
    "        return \"AP \" + (\"Last\" if \"last\" in label else label.split(\"@\")[-1])\n",
    "    else:\n",
    "        return label\n",
    "\n",
    "def format_layer_number(label: str) -> str:\n",
    "    if \"last\" in label:\n",
    "        return \"Last\"\n",
    "    return label.split(\"@\")[-1]\n",
    "\n",
    "    \n",
    "# Desired row order (match your table exactly, but use df's dataset names)\n",
    "dataset_order = [\n",
    "    # Natural (MD)\n",
    "    \"STL-10\", \"CIFAR-10\", \"Caltech-101\", \"PASCAL VOC 2007\", \"CIFAR-100\", \"Country-211\",\n",
    "    # Natural (SD)\n",
    "    \"Pets\", \"Flowers\", \"Stanford Cars\", \"FGVC Aircraft\", \"GTSRB\", \"SVHN\",\n",
    "    # Specialized \"PCAM\",\n",
    "    \"EuroSAT\", \"RESISC45\", \"Diabetic Retinopathy\",\n",
    "    # Structured\n",
    "    \"DTD\", \"FER2013\", \"Dmlab\"\n",
    "]\n",
    "# Indices where category changes occur (lengths of groups above)\n",
    "category_breaks = {\n",
    "    \"Natural (MD)\": 6,\n",
    "    \"Natural (SD)\": 12,\n",
    "    \"Specialized\": 15,\n",
    "    \"Structured\": 18\n",
    "}\n",
    "palette_list = list(plt.cm.tab20c.colors)\n",
    "palette_list = palette_list[4:8][::-1]\n",
    "# Prepend white to the palette\n",
    "palette_with_white = [(1.,1.,1.), *palette_list]\n",
    "multi_color_cmap = LinearSegmentedColormap.from_list(\n",
    "    \"multi_gradient_with_white\", \n",
    "    palette_with_white\n",
    ")\n",
    "\n",
    "model_name_mapping = {\"Clip\":\"CLIP-B-16\",\"Dinov2\": \"DINOv2-B-14\",  \"ViT-B\":\"ViT-B-16\",\n",
    "                      'dinov2-vit-base-p14': \"DINOv2-B-14\",\n",
    "                      'OpenCLIP_ViT-B-16_openai':\"CLIP-B-16\",\n",
    "                       'vit_base_patch16_224':\"ViT-B-16\",\n",
    "                     'dinov2-vit-small-p14':\"DINOv2-S-14\",'OpenCLIP_ViT-B-32_openai':\"CLIP-B-32\",'vit_small_patch16_224':\"ViT-S-16\",\n",
    "                      'dinov2-vit-large-p14':\"DINOv2-L-14\",'OpenCLIP_ViT-L-14_openai':\"CLIP-L-16\",'vit_large_patch16_224':\"ViT-L-16\"\n",
    "                      }\n",
    "model_name_mapping = base_model_name_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cd14a1-4b35-48a2-947e-a1820130bed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info = load_ds_info(ds_info_file)\n",
    "# all_runs_path_large = BASE_PATH_PROJECT / f'results_{FOLDER_SUBSTRING}_exp/aggregated/all_runs_v10.pkl'\n",
    "all_runs_path = BASE_PATH_PROJECT / f'results_{FOLDER_SUBSTRING}_end2end_finetuning/aggregated/complete_set_of_run.pkl'\n",
    "all_runs = pd.read_pickle(all_runs_path)\n",
    "# Filter \n",
    "all_runs_large= all_runs[all_runs[\"base_model_fmt\"].isin([\"MAE-B-16\",\"MAE-L-16\"]) &\\\n",
    "    all_runs[\"task\"].isin([\"attentive_probe\"]) &\\\n",
    "    all_runs[\"nr_layers\"].isin([24,48])\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d2d354-34d5-4ba8-9392-cdc311daea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "attn_path = BASE_PATH_PROJECT / 'results/plots'\n",
    "\n",
    "def get_attn_weights(model_list,save_name=\"total_attn_weights_large.pkl\"):\n",
    "    try:\n",
    "        with open(attn_path / save_name, \"rb\") as f:\n",
    "            total_attn_weights = pickle.load(f)\n",
    "        print(\"Just loaded weights\")\n",
    "    except:\n",
    "        \n",
    "        total_attn_weights = {}\n",
    "        for base_model in model_list:\n",
    "            df = all_runs_large[all_runs_large[\"base_model\"]==base_model]\n",
    "            full_attn_weights = {}\n",
    "            for index, row in df.iterrows():\n",
    "                ds = row[\"dataset\"].replace(\"/\",\"_\")\n",
    "                if \"imagenet\" in ds:\n",
    "                    continue\n",
    "                feature_dir = base_feature_dir / ds\n",
    "                names = eval(row[\"model_ids\"])\n",
    "                # sort by group (ap/cls), then by block number\n",
    "                sorted_names = sorted(names, key=lambda x: ((\"cls\" in x), extract_block_key(x)))\n",
    "                # get new sorted indices\n",
    "                sorted_indices = [names.index(s) for s in sorted_names]\n",
    "                \n",
    "                feature_dirs = [feature_dir / mid.replace('@',\"/\") for mid in names]\n",
    "                #print(feature_dirs)\n",
    "                if np.isnan(row[\"dim\"]):\n",
    "                    print(\"Somehow Nan in row\",row)\n",
    "                    continue\n",
    "                #print(int(row[\"dim\"]))\n",
    "                _, feature_test_loader = get_combined_feature_dl(\n",
    "                        feature_dirs=feature_dirs,\n",
    "                        batch_size = 2048,\n",
    "                        num_workers=0,\n",
    "                        fewshot_k=-1,\n",
    "                        feature_combiner_cls=partial(StackedZeroPadFeatureCombiner, shared_dim=int(row[\"dim\"])),\n",
    "                        normalize = True,\n",
    "                        load_train = False,\n",
    "                    )\n",
    "                model_path = base_model_dir / ds / row[\"model_id_n_hopt_slug\"] / \"model.pkl\"\n",
    "                model = load_model_from_file(\n",
    "                    model_path = model_path, \n",
    "                    device = device\n",
    "                )\n",
    "                model.eval()\n",
    "                test_attn_weigths = get_attention_weights(model, feature_test_loader).squeeze().mean(axis=0)[:,sorted_indices]\n",
    "                full_attn_weights[ds] = (sorted_names, test_attn_weigths)\n",
    "            total_attn_weights[base_model] = full_attn_weights\n",
    "        with open(attn_path / save_name, \"wb\") as f:\n",
    "            pickle.dump(total_attn_weights, f)\n",
    "    return total_attn_weights\n",
    "\n",
    "total_attn_weights_mae = get_attn_weights([ \"mae-vit-base-p16\", \"mae-vit-large-p16\"],\n",
    "                                                \"total_attn_weights_mae.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e3b04d-00cb-4d67-a355-ac7cca550f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = {}\n",
    "records = []\n",
    "for model_name, model_results in total_attn_weights_mae.items():\n",
    "    for ds, (layer_names, attn) in model_results.items():\n",
    "        if \"imagenet\" in ds:\n",
    "            print(\"No Imagenet\")\n",
    "            continue\n",
    "        # Mean over Heads:\n",
    "        attn_mean = attn.mean(axis=0)  \n",
    "        for i, lname in enumerate(layer_names):\n",
    "            token_type = \"CLS\" if \"cls\" in lname else \"AVG\"   # split CLS vs AVG\n",
    "            if \"Clip\" in model_name or \"CLIP\" in model_name:\n",
    "                layer_id = lname.split(\"openai_\")[-1].replace(\"visual.transformer.resblocks.\",\"\").replace(\".ln_2\",\"\").replace(\"visual\",\"last\")\n",
    "                #print(layer_id)\n",
    "            else:\n",
    "                layer_id = lname.split(\"_\")[-1].replace(\".norm2\",\"\").replace(\"norm\",\"last\").replace(\"blocks.\",\"\")\n",
    "            records.append([model_name, ds, token_type, layer_id, attn_mean[i],i])\n",
    "        \n",
    "df = pd.DataFrame(records, columns=[\"Model\",\"Dataset\",\"TokenType\",\"Layer\",\"Attention\",\"layer_idx\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dbde18-aebe-43cf-9717-3ce1c0424b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "mean_per_dataset = df.groupby([\"Dataset\",\"Layer\"])[\"Attention\"].mean().reset_index()\n",
    "pivot = mean_per_dataset.pivot_table(index=\"Dataset\", columns=\"Layer\", values=\"Attention\")\n",
    "#sorted_cols = sorted(pivot.columns, key=lambda x: ((\"cls\" in str(x), \"last\" in str(x)), 999 if \"last\" in x else int(x.split(\"@\")[-1])))\n",
    "sorted_cols = sorted(pivot.columns, key=layer_sort_key)\n",
    "\n",
    "#sorted_cols = sorted(pivot.columns, key=lambda x: ((\"cls\" in str(x), \"visual\" in str(x)), 999 if \"visual\" in x else int(x.split(\"@\")[-1]) ))\n",
    "pivot = pivot[sorted_cols]\n",
    "from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "\n",
    "# Compute linkage on dataset rows\n",
    "row_linkage = linkage(pivot.fillna(0), method=\"ward\")\n",
    "row_order = leaves_list(row_linkage)\n",
    "\n",
    "# Reorder datasets by dendrogram\n",
    "pivot = pivot.iloc[row_order]\n",
    "\n",
    "sns.heatmap(pivot, cmap=multi_color_cmap,)#\"viridis\")\n",
    "if False:\n",
    "    sns.clustermap(\n",
    "        pivot,\n",
    "        cmap=\"viridis\",\n",
    "        row_cluster=True, col_cluster=False,  # cluster only datasets\n",
    "        figsize=(10,8)\n",
    "    )\n",
    "plt.suptitle(\"Layer Attention Heatmap (Datasets Clustered by Similarity)\", y=1.02)\n",
    "#plt.show()\n",
    "#plt.title(\"Layer Attention Heatmap (per dataset)\")\n",
    "#fn = BASE_PATH_PROJECT / f\"results_{FOLDER_SUBSTRING}_rebuttal/plots\"\n",
    "#plt.savefig(fn / \"MAEAttentionMapwoIN.png\",bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f338cf4c-987e-4974-ab3f-b56edd21de82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#size = \"Large\"\n",
    "#df = all_dfs[size]\n",
    "# --- Aggregate per Model × Dataset × Layer ---\n",
    "mean_per_dataset = (\n",
    "    df.groupby([\"Model\",\"Dataset\",\"Layer\"])[\"Attention\"]\n",
    "      .mean()\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "models = df[\"Model\"].unique()\n",
    "models = sorted(models)\n",
    "# --- Compute global vmin/vmax for shared colormap ---\n",
    "all_values = mean_per_dataset[\"Attention\"].values\n",
    "vmin, vmax = all_values.min(), all_values.max()\n",
    "\n",
    "#fig, axes = plt.subplots(1, len(models), figsize=(9*len(models), 6), sharey=True)\n",
    "fig, axes = plt.subplots(1, len(models), figsize=(9*len(models), 8), sharey=True)\n",
    "fig.subplots_adjust(wspace=0.05)\n",
    "if len(models) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "heatmaps = []\n",
    "for ax, model_name in zip(axes, models):\n",
    "    pivot = mean_per_dataset.query(\"Model == @model_name\").pivot(\n",
    "        index=\"Dataset\", columns=\"Layer\", values=\"Attention\"\n",
    "    )\n",
    "    pivot = pivot.rename(index=lambda ds: data_info.loc[\n",
    "        ds.replace(\"_\",\"/\")\n",
    "          .replace(\"fgvc/air\",\"fgvc_air\")\n",
    "          .replace(\"diabetic/retinopathy\",\"diabetic_retinopathy\"), \n",
    "        \"name\"\n",
    "    ])\n",
    "\n",
    "   \n",
    "    sorted_cols = sorted(pivot.columns, key=layer_sort_key)\n",
    "    pivot = pivot[sorted_cols]\n",
    "    # Reorder datasets by dendrogram\n",
    "    #pivot = pivot.iloc[row_order]\n",
    "    pivot = pivot.reindex(dataset_order)\n",
    "\n",
    "    # --- Plot heatmap ---\n",
    "    hm = sns.heatmap(\n",
    "        pivot, cmap=multi_color_cmap,#\"viridis\",\n",
    "        vmin=vmin, vmax=vmax,\n",
    "        cbar=False, ax=ax\n",
    "    )\n",
    "    heatmaps.append(hm)\n",
    "\n",
    "    ax.set_title(f\"{model_name_mapping[model_name]}\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    if False: #ax == axes[0]:\n",
    "        ax.set_ylabel(\"Dataset\")\n",
    "    else:\n",
    "        ax.set_ylabel(\"\")\n",
    "\n",
    "    # --- Force all x-tick labels visible ---\n",
    "    if len(pivot.columns) <=24:\n",
    "        ax.set_xticks(np.arange(1,len(pivot.columns),2) + 0.5)\n",
    "        #ax.set_xticklabels([format_layer_number(col) for col in pivot.columns], rotation=45, ha=\"right\")\n",
    "        ax.set_xticklabels([format_layer_number(col) for col in pivot.columns[1::2]], rotation=0)\n",
    "    else:\n",
    "        ticks_range = np.asarray([0] + list(np.arange(3,len(pivot.columns),4)))\n",
    "        ax.set_xticks(ticks_range + 0.5)\n",
    "        formated_col = [format_layer_number(col) for col in pivot.columns]\n",
    "        print(formated_col[3::4])\n",
    "        ax.set_xticklabels(formated_col[:1] + formated_col[3::4], rotation=0)\n",
    "\n",
    "    #ax.set_xticklabels([format_layer_label(col) for col in pivot.columns], rotation=45, ha=\"right\")\n",
    "    #ax.set_xticklabels(pivot.columns, rotation=45, ha=\"right\")#, fontsize=fontsizes_cols[\"ticks\"])\n",
    "    #ax.set_yticklabels(pivot.index.map(lambda ds: data_info.loc[ds.replace(\"_\",\"/\").replace(\"fgvc/air\",\"fgvc_air\").replace(\"diabetic/retinopathy\",\"diabetic_retinopathy\"), \"name\"]))\n",
    "    ax.set_yticklabels(pivot.index)\n",
    "    # Make cls@last bold\n",
    "    for label in ax.get_xticklabels():\n",
    "        if any(tag in label.get_text() for tag in [\"Last\"]):\n",
    "            #label.set_fontweight(\"bold\")\n",
    "            label.set_rotation(90)   \n",
    "\n",
    "    n_cls = sum(col.startswith(\"cls@\") for col in pivot.columns)\n",
    "    n_ap = sum(col.startswith(\"ap@\") for col in pivot.columns)\n",
    "    \n",
    "    # Add horizontal separator lines\n",
    "    for _, row_idx in list(category_breaks.items())[:-1]:  # skip last\n",
    "        ax.hlines(row_idx, *ax.get_xlim(), colors=\"grey\",ls=\"--\", linewidth=1)\n",
    "    ax.vlines(n_cls,*ax.get_ylim(),colors=\"grey\", ls=\"--\",linewidth=1)\n",
    "\n",
    "    \n",
    "    # y position: a bit below the current x-axis labels\n",
    "    ypos = -0.08\n",
    "    ax.text(n_cls/2/len(pivot.columns), ypos, \"CLS\", ha=\"center\", va=\"top\", fontsize=22,fontweight=\"bold\", transform=ax.transAxes)\n",
    "    ax.text((n_cls + n_ap/2)/len(pivot.columns), ypos, \"AP\", ha=\"center\", va=\"top\", fontsize=22,fontweight=\"bold\", transform=ax.transAxes)\n",
    "\n",
    "# --- Single shared colorbar ---\n",
    "cbar = fig.colorbar(heatmaps[-1].collections[0], ax=axes, location=\"right\", fraction=0.02, pad=0.02)\n",
    "cbar.set_label(\"Average Attention over Heads\")\n",
    "\n",
    "#plt.suptitle(\"Layer Attention Heatmaps per Model\", y=1.02)\n",
    "fn = BASE_PATH_PROJECT / f\"results_{FOLDER_SUBSTRING}_rebuttal/plots\"\n",
    "plt.savefig(fn / \"MAEAttentionMapOverDatasetPerModelswoIN.png\",dpi=600,bbox_inches='tight')\n",
    "#plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rep2rep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
